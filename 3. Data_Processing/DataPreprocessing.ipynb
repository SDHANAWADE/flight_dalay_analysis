{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .load(\"s3://vitaproject23/cleandata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to predict delay at the time of Ticket booking and hence the following columns wont help us in prediction because customer will be unaware of the following data :- 'TAXI_OUT', 'TAXI_IN', 'WHEELS_OFF', 'WHEELS_ON', 'ARR_DELAY', 'DEP_DELAY', 'ACTUAL_ELAPSED_TIME', 'DEP_TIME', 'ARR_TIME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop('FL_DATE','TAXI_OUT','WHEELS_OFF','WHEELS_ON','TAXI_IN','ARR_DELAY','DEP_DELAY','ACTUAL_ELAPSED_TIME','DEP_TIME','ARR_TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OP_CARRIER', 'ORIGIN', 'DEST', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'CRS_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'MONTH', 'WEEKDAY', 'YEAR', 'FLIGHT_STATUS']"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(\"Number of Columns: \", len(df.columns))\n",
    "print(\"Number of Rows:\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,VectorAssembler,VectorIndexer,StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converting Categorical Columns such as 'OP_CARRIER', 'ORIGIN', 'DEST' are converted into Indexed Columns 'OP_CARRIER_I', 'ORIGIN_I', 'DEST_I' using StringIndexer.\n",
    "- StringIndexer is used to convert categorical columns to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexer1 = StringIndexer(inputCol='OP_CARRIER',outputCol='OP_CARRIER_I')\n",
    "strindexedDF1 = indexer1.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer2 = StringIndexer(inputCol='ORIGIN',outputCol='ORIGIN_I')\n",
    "strindexedDF2 = indexer2.fit(strindexedDF1).transform(strindexedDF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer3 = StringIndexer(inputCol='DEST',outputCol='DEST_I')\n",
    "strindexedDF3 = indexer3.fit(strindexedDF2).transform(strindexedDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----+------------+------------+----------------+--------+--------+-----+-------+----+-------------+------------+--------+------+\n",
      "|OP_CARRIER     |ORIGIN|DEST|CRS_DEP_TIME|CRS_ARR_TIME|CRS_ELAPSED_TIME|AIR_TIME|DISTANCE|MONTH|WEEKDAY|YEAR|FLIGHT_STATUS|OP_CARRIER_I|ORIGIN_I|DEST_I|\n",
      "+---------------+------+----+------------+------------+----------------+--------+--------+-----+-------+----+-------------+------------+--------+------+\n",
      "|United Airlines|ORD   |OMA |3           |0           |87.0            |66.0    |416.0   |6    |2      |2011|1            |4.0         |1.0     |60.0  |\n",
      "|United Airlines|DEN   |IAD |2           |3           |198.0           |171.0   |1452.0  |6    |2      |2011|0            |4.0         |3.0     |27.0  |\n",
      "|United Airlines|RNO   |DEN |2           |2           |129.0           |99.0    |804.0   |6    |2      |2011|0            |4.0         |66.0    |3.0   |\n",
      "|United Airlines|LAX   |SFO |1           |1           |86.0            |54.0    |337.0   |6    |2      |2011|1            |4.0         |4.0     |7.0   |\n",
      "|United Airlines|SFO   |IAD |2           |3           |311.0           |264.0   |2419.0  |6    |2      |2011|0            |4.0         |7.0     |27.0  |\n",
      "+---------------+------+----+------------+------------+----------------+--------+--------+-----+-------+----+-------------+------------+--------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "strindexedDF3.select('OP_CARRIER','OP_CARRIER_I','ORIGIN','ORIGIN_I','DEST','DEST_I').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OP_CARRIER', 'ORIGIN', 'DEST', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'CRS_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'MONTH', 'WEEKDAY', 'YEAR', 'FLIGHT_STATUS', 'OP_CARRIER_I', 'ORIGIN_I', 'DEST_I']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning models in Spark expects all features in single column. Therefore VectorAssembler combines all features and gives us vector which can be stored in single column. \n",
    "- VectorAssembler expects only Numerical Features, hence we do not take into account 'OP_CARRRIER', 'ORIGIN' and 'DEST'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list of all the variables that you want to create feature vectors\n",
    "# These features are then further used for training model\n",
    "features_col = ['CRS_DEP_TIME', 'CRS_ARR_TIME', 'CRS_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', \n",
    "                'MONTH', 'WEEKDAY', 'OP_CARRIER_I', 'ORIGIN_I', 'DEST_I']\n",
    "\n",
    "assembler = VectorAssembler(inputCols= features_col, outputCol= \"features\")\n",
    "assembledDF = assembler.transform(strindexedDF3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|features                                         |\n",
      "+-------------------------------------------------+\n",
      "|[3.0,0.0,87.0,66.0,416.0,6.0,2.0,4.0,1.0,60.0]   |\n",
      "|[2.0,3.0,198.0,171.0,1452.0,6.0,2.0,4.0,3.0,27.0]|\n",
      "|[2.0,2.0,129.0,99.0,804.0,6.0,2.0,4.0,66.0,3.0]  |\n",
      "|[1.0,1.0,86.0,54.0,337.0,6.0,2.0,4.0,4.0,7.0]    |\n",
      "|[2.0,3.0,311.0,264.0,2419.0,6.0,2.0,4.0,7.0,27.0]|\n",
      "+-------------------------------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "assembledDF.select(\"features\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After VectorAssembler next step is VectorIndexer. \n",
    "- Vector Indexer Automatically identifies categorical features from the feature vector (Output Column of VectorAssembler) and then indexes those categorical features inside vector. \n",
    "- VectorIndexer let usskip OneHotEncoding stage for encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vecindexer = VectorIndexer(inputCol= \"features\", outputCol= \"indexed_features\")\n",
    "vecindexedDF = vecindexer.fit(assembledDF).transform(assembledDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------------------------------------+\n",
      "|features                                         |indexed_features                                 |\n",
      "+-------------------------------------------------+-------------------------------------------------+\n",
      "|[3.0,0.0,87.0,66.0,416.0,6.0,2.0,4.0,1.0,60.0]   |[3.0,0.0,87.0,66.0,416.0,5.0,1.0,4.0,1.0,60.0]   |\n",
      "|[2.0,3.0,198.0,171.0,1452.0,6.0,2.0,4.0,3.0,27.0]|[2.0,3.0,198.0,171.0,1452.0,5.0,1.0,4.0,3.0,27.0]|\n",
      "|[2.0,2.0,129.0,99.0,804.0,6.0,2.0,4.0,66.0,3.0]  |[2.0,2.0,129.0,99.0,804.0,5.0,1.0,4.0,66.0,3.0]  |\n",
      "|[1.0,1.0,86.0,54.0,337.0,6.0,2.0,4.0,4.0,7.0]    |[1.0,1.0,86.0,54.0,337.0,5.0,1.0,4.0,4.0,7.0]    |\n",
      "|[2.0,3.0,311.0,264.0,2419.0,6.0,2.0,4.0,7.0,27.0]|[2.0,3.0,311.0,264.0,2419.0,5.0,1.0,4.0,7.0,27.0]|\n",
      "|[1.0,1.0,324.0,291.0,2288.0,6.0,2.0,4.0,27.0,4.0]|[1.0,1.0,324.0,291.0,2288.0,5.0,1.0,4.0,27.0,4.0]|\n",
      "|[1.0,1.0,127.0,104.0,651.0,6.0,2.0,4.0,5.0,7.0]  |[1.0,1.0,127.0,104.0,651.0,5.0,1.0,4.0,5.0,7.0]  |\n",
      "|[1.0,2.0,82.0,61.0,406.0,6.0,2.0,4.0,3.0,151.0]  |[1.0,2.0,82.0,61.0,406.0,5.0,1.0,4.0,3.0,151.0]  |\n",
      "|[1.0,1.0,101.0,76.0,495.0,6.0,2.0,4.0,61.0,3.0]  |[1.0,1.0,101.0,76.0,495.0,5.0,1.0,4.0,61.0,3.0]  |\n",
      "|[1.0,2.0,127.0,90.0,679.0,6.0,2.0,4.0,7.0,16.0]  |[1.0,2.0,127.0,90.0,679.0,5.0,1.0,4.0,7.0,16.0]  |\n",
      "|[1.0,1.0,93.0,68.0,372.0,6.0,2.0,4.0,42.0,7.0]   |[1.0,1.0,93.0,68.0,372.0,5.0,1.0,4.0,42.0,7.0]   |\n",
      "|[1.0,1.0,245.0,213.0,1557.0,6.0,2.0,4.0,21.0,3.0]|[1.0,1.0,245.0,213.0,1557.0,5.0,1.0,4.0,21.0,3.0]|\n",
      "|[1.0,2.0,291.0,260.0,2288.0,6.0,2.0,4.0,4.0,27.0]|[1.0,2.0,291.0,260.0,2288.0,5.0,1.0,4.0,4.0,27.0]|\n",
      "|[3.0,3.0,158.0,110.0,733.0,6.0,2.0,4.0,18.0,1.0] |[3.0,3.0,158.0,110.0,733.0,5.0,1.0,4.0,18.0,1.0] |\n",
      "|[2.0,3.0,169.0,139.0,1024.0,6.0,2.0,4.0,3.0,16.0]|[2.0,3.0,169.0,139.0,1024.0,5.0,1.0,4.0,3.0,16.0]|\n",
      "|[1.0,1.0,139.0,107.0,862.0,6.0,2.0,4.0,4.0,3.0]  |[1.0,1.0,139.0,107.0,862.0,5.0,1.0,4.0,4.0,3.0]  |\n",
      "|[2.0,3.0,140.0,113.0,888.0,6.0,2.0,4.0,3.0,1.0]  |[2.0,3.0,140.0,113.0,888.0,5.0,1.0,4.0,3.0,1.0]  |\n",
      "|[2.0,2.0,111.0,96.0,629.0,6.0,2.0,4.0,8.0,3.0]   |[2.0,2.0,111.0,96.0,629.0,5.0,1.0,4.0,8.0,3.0]   |\n",
      "|[2.0,3.0,290.0,263.0,2399.0,6.0,2.0,4.0,33.0,7.0]|[2.0,3.0,290.0,263.0,2399.0,5.0,1.0,4.0,33.0,7.0]|\n",
      "|[1.0,1.0,352.0,328.0,2419.0,6.0,2.0,4.0,27.0,7.0]|[1.0,1.0,352.0,328.0,2419.0,5.0,1.0,4.0,27.0,7.0]|\n",
      "+-------------------------------------------------+-------------------------------------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "vecindexedDF.select(\"features\", \"indexed_features\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- WEEKDAY: integer (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- FLIGHT_STATUS: integer (nullable = true)\n",
      " |-- OP_CARRIER_I: double (nullable = false)\n",
      " |-- ORIGIN_I: double (nullable = false)\n",
      " |-- DEST_I: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- indexed_features: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "vecindexedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler scales each value in the feature vector such that the mean is 0 and the standard deviation is 1\n",
    "- It takes parameters:\n",
    "    - withStd: True by default. Scales the data to unit standard deviation\n",
    "    - withMean: False by default. Centers the data with mean before scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stdscaler = StandardScaler(inputCol= \"indexed_features\", outputCol= \"scaledfeatures\")\n",
    "stdscaledDF = stdscaler.fit(vecindexedDF).transform(vecindexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaledfeatures                                                                                                                                                                                |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[3.7759086415361205,0.0,1.1849301019673635,0.9341508357077831,0.7000001868483029,1.4752922180737393,0.5104440799032741,0.827719427843689,0.02166320217560367,1.2991170971063506]              |\n",
      "|[2.517272427690747,3.6297005740292247,2.6967374734429654,2.42029989251562,2.4432698829416726,1.4752922180737393,0.5104440799032741,0.827719427843689,0.06498960652681102,0.5846026936978578]  |\n",
      "|[2.517272427690747,2.41980038268615,1.7569653236067804,1.4012262535616749,1.3528849765048931,1.4752922180737393,0.5104440799032741,0.827719427843689,1.4297713435898423,0.06495585485531753]  |\n",
      "|[1.2586362138453735,1.209900191343075,1.1713102157378537,0.764305229215459,0.5670674590573992,1.4752922180737393,0.5104440799032741,0.827719427843689,0.08665280870241468,0.15156366132907423]|\n",
      "|[2.517272427690747,3.6297005740292247,4.235784617377587,3.7366033428311325,4.070433778812608,1.4752922180737393,0.5104440799032741,0.827719427843689,0.1516424152292257,0.5846026936978578]   |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stdscaledDF.select(\"scaledfeatures\" ).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stdscaledDF.write.option(\"header\",\"true\").parquet(\"s3://project.group23.fan5.flight/Scaled_Data_M/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train the model with all data except YEAR 2018 data because that is used to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test data on 2018 \n",
    "from pyspark.sql.functions import col\n",
    "trainDF = stdscaledDF.where(col(\"YEAR\")!=2018)\n",
    "testDF = stdscaledDF.where(col(\"YEAR\")==2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- WEEKDAY: integer (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- FLIGHT_STATUS: integer (nullable = true)\n",
      " |-- OP_CARRIER_I: double (nullable = false)\n",
      " |-- ORIGIN_I: double (nullable = false)\n",
      " |-- DEST_I: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- indexed_features: vector (nullable = true)\n",
      " |-- scaledfeatures: vector (nullable = true)"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set =  53613134\n",
      "Observations in testing set =  7076405"
     ]
    }
   ],
   "source": [
    "# print the count of observations in each set\n",
    "print(\"NUmber of Observations in training set = \", trainDF.count())\n",
    "print(\"Number of Observations in testing set = \", testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      ".\n",
      "22/03/26 12:59:18 WARN TaskSetManager: Lost task 3.0 in stage 51.0 (TID 287, ip-172-31-77-8.ec2.internal, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container from a bad node: container_1648292310456_0003_01_000024 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 12:59:18.552]Container killed on request. Exit code is 137\n",
      "[2022-03-26 12:59:18.552]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 12:59:18.553]Killed by external signal\n",
      ".\n",
      "22/03/26 12:59:18 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 12 for reason Container from a bad node: container_1648292310456_0003_01_000024 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 12:59:18.552]Container killed on request. Exit code is 137\n",
      "[2022-03-26 12:59:18.552]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 12:59:18.553]Killed by external signal\n",
      ".\n",
      "22/03/26 12:59:18 INFO BlockManagerMaster: Removal of executor 12 requested\n",
      "22/03/26 12:59:18 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 12\n",
      "22/03/26 12:59:18 INFO BlockManagerMasterEndpoint: Trying to remove executor 12 from BlockManagerMaster.\n",
      "22/03/26 12:59:18 INFO ExecutorAllocationManager: Existing executor 12 has been removed (new total is 1)\n",
      "22/03/26 12:59:18 WARN ExecutorAllocationManager: Attempted to mark unknown executor 12 idle\n",
      "22/03/26 12:59:20 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.77.8:49230) with ID 13\n",
      "22/03/26 12:59:20 INFO ExecutorAllocationManager: New executor 13 has registered (new total is 2)\n",
      "22/03/26 12:59:20 INFO TaskSetManager: Starting task 3.1 in stage 51.0 (TID 289, ip-172-31-77-8.ec2.internal, executor 13, partition 3, RACK_LOCAL, 8379 bytes)\n",
      "22/03/26 12:59:20 INFO TaskSetManager: Starting task 2.1 in stage 51.0 (TID 290, ip-172-31-77-8.ec2.internal, executor 13, partition 2, RACK_LOCAL, 8560 bytes)\n",
      "22/03/26 12:59:21 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-77-8.ec2.internal:32805 with 4.8 GB RAM, BlockManagerId(13, ip-172-31-77-8.ec2.internal, 32805, None)\n",
      "22/03/26 12:59:21 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on ip-172-31-77-8.ec2.internal:32805 (size: 121.4 KB, free: 4.8 GB)\n",
      "22/03/26 12:59:54 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on ip-172-31-77-8.ec2.internal:32805 (size: 11.4 KB, free: 4.8 GB)\n",
      "22/03/26 13:00:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 13.\n",
      "22/03/26 13:00:44 INFO DAGScheduler: Executor lost: 13 (epoch 16)\n",
      "22/03/26 13:00:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 13 from BlockManagerMaster.\n",
      "22/03/26 13:00:44 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(13, ip-172-31-77-8.ec2.internal, 32805, None)\n",
      "22/03/26 13:00:44 INFO BlockManagerMaster: Removed 13 successfully in removeExecutor\n",
      "22/03/26 13:00:44 ERROR YarnScheduler: Lost executor 13 on ip-172-31-77-8.ec2.internal: Container from a bad node: container_1648292310456_0003_01_000025 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 13:00:44.455]Container killed on request. Exit code is 137\n",
      "[2022-03-26 13:00:44.455]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 13:00:44.456]Killed by external signal\n",
      ".\n",
      "22/03/26 13:00:44 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 13 for reason Container from a bad node: container_1648292310456_0003_01_000025 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 13:00:44.455]Container killed on request. Exit code is 137\n",
      "[2022-03-26 13:00:44.455]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 13:00:44.456]Killed by external signal\n",
      ".\n",
      "22/03/26 13:00:44 WARN TaskSetManager: Lost task 3.1 in stage 51.0 (TID 289, ip-172-31-77-8.ec2.internal, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1648292310456_0003_01_000025 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 13:00:44.455]Container killed on request. Exit code is 137\n",
      "[2022-03-26 13:00:44.455]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 13:00:44.456]Killed by external signal\n",
      ".\n",
      "22/03/26 13:00:44 WARN TaskSetManager: Lost task 2.1 in stage 51.0 (TID 290, ip-172-31-77-8.ec2.internal, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container from a bad node: container_1648292310456_0003_01_000025 on host: ip-172-31-77-8.ec2.internal. Exit status: 137. Diagnostics: [2022-03-26 13:00:44.455]Container killed on request. Exit code is 137\n",
      "[2022-03-26 13:00:44.455]Container exited with a non-zero exit code 137. \n",
      "[2022-03-26 13:00:44.456]Killed by external signal\n",
      ".\n",
      "22/03/26 13:00:44 INFO BlockManagerMaster: Removal of executor 13 requested\n",
      "22/03/26 13:00:44 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 13\n",
      "22/03/26 13:00:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 13 from BlockManagerMaster.\n",
      "22/03/26 13:00:44 INFO ExecutorAllocationManager: Existing executor 13 has been removed (new total is 1)\n",
      "22/03/26 13:00:47 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.77.8:49260) with ID 14\n",
      "22/03/26 13:00:47 INFO TaskSetManager: Starting task 2.2 in stage 51.0 (TID 291, ip-172-31-77-8.ec2.internal, executor 14, partition 2, RACK_LOCAL, 8560 bytes)\n",
      "22/03/26 13:00:47 INFO TaskSetManager: Starting task 3.2 in stage 51.0 (TID 292, ip-172-31-77-8.ec2.internal, executor 14, partition 3, RACK_LOCAL, 8379 bytes)\n",
      "22/03/26 13:00:47 INFO ExecutorAllocationManager: New executor 14 has registered (new total is 2)\n",
      "22/03/26 13:00:47 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-77-8.ec2.internal:35803 with 4.8 GB RAM, BlockManagerId(14, ip-172-31-77-8.ec2.internal, 35803, None)\n",
      "22/03/26 13:00:47 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on ip-172-31-77-8.ec2.internal:35803 (size: 121.4 KB, free: 4.8 GB)\n",
      "22/03/26 13:01:24 INFO TaskSetManager: Starting task 5.0 in stage 51.0 (TID 293, ip-172-31-66-176.ec2.internal, executor 9, partition 5, PROCESS_LOCAL, 8379 bytes)\n",
      "22/03/26 13:01:24 INFO TaskSetManager: Finished task 4.0 in stage 51.0 (TID 288) in 159777 ms on ip-172-31-66-176.ec2.internal (executor 9) (2/10)\n",
      "22/03/26 13:01:26 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state FAILED! Check the YARN application logs for more details.\n",
      "22/03/26 13:01:26 ERROR YarnClientSchedulerBackend: Diagnostics message: Application application_1648292310456_0003 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1648292310456_0003_000001 exited with  exitCode: -100\n",
      "Failing this attempt.Diagnostics: Container released on a *lost* nodeFor more detailed output, check the application tracking page: http://ip-172-31-76-239.ec2.internal:8088/cluster/app/application_1648292310456_0003 Then click on links to logs of each attempt.\n",
      ". Failing the application.\n",
      "22/03/26 13:01:26 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-76-239.ec2.internal:4041\n",
      "22/03/26 13:01:26 INFO DAGScheduler: Job 35 failed: collectAsMap at RandomForest.scala:567, took 201.336362 s\n",
      "22/03/26 13:01:26 INFO DAGScheduler: ShuffleMapStage 51 (mapPartitions at RandomForest.scala:538) failed in 201.334 s due to Stage cancelled because SparkContext was shut down\n",
      "22/03/26 13:01:26 ERROR Instrumentation: org.apache.spark.SparkException: Job 35 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:1000)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:998)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:998)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2408)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2315)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1985)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1385)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1984)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:121)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:805)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2118)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2162)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:142)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:120)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:120)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n"
     ]
    }
   ],
   "source": [
    "# import the RandomForestClassifier function from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Build the RandomForestClassifier object 'dt' by setting the required parameters\n",
    "# We will pass the VectorIndexed columns as featureCol for Random Forest. Since they can handle categorical indexes\n",
    "rf = RandomForestClassifier(featuresCol=\"scaledfeatures\", labelCol=\"FLIGHT_STATUS\" , numTrees=100, maxDepth= 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the RandomForestClassifier object on the training data\n",
    "rfmodel = rf.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This RandomForestClassifierModel can be used as a transformer to perform prediction on the testing data\n",
    "rfpredictonDF = rfmodel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpredictonDF.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy\n",
    "print(\"Accuracy: \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"accuracy\"})) \n",
    "# 2. Area under the ROC curve\n",
    "print('Area under the ROC curve = ', evaluator.evaluate(rfpredictonDF))\n",
    "# 3. Precision (Positive Predictive Value)\n",
    "print(\"Precision = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"weightedPrecision\"}))\n",
    "# 4. Recall (True Positive Rate)\n",
    "print(\"Recall = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"weightedRecall\"}))\n",
    "# 5. F1 Score (F-measure)\n",
    "print(\"F1 Score = \", multievaluator.evaluate(rfpredictonDF, {evaluator.metricName: \"f1\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
